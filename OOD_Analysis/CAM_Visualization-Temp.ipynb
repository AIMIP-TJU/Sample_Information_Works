{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorwatch as tw\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils.dataset import *\n",
    "import torch.utils.data as data\n",
    "\n",
    "from utils.train import *\n",
    "from utils.test  import *\n",
    "from utils.model_select import model_select\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "####### test作训练集 #######\n",
    "# epochs = 80\n",
    "# lr = 3e-6\n",
    "# gamma = 0.7\n",
    "# step_size = 5\n",
    "###### train作训练集 #######\n",
    "epochs = 5\n",
    "lr = 2e-6\n",
    "gamma = 0.8\n",
    "step_size = 2\n",
    "\n",
    "seed = 42\n",
    "device = 'cpu'\n",
    "\n",
    "file_Path = '/home/a611/Projects/Datasets/NICO/'\n",
    "train_name = ['/home/a611/Projects/Datasets/NICO/labels/NICO_animal_train.csv']\n",
    "test_name = ['/home/a611/Projects/gyc/Local_Features/csvs/OOD_without_cage_monkey.csv']\n",
    "num_classes = 10\n",
    "num_input = 3\n",
    "batch_size = 1\n",
    "num_workers = 2\n",
    "########################\n",
    "os.chdir('examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEIyZaYXRkZF"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YKR5RaykR58x"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0JAXWWtSXpB"
   },
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pC-D9a8xSZNd"
   },
   "outputs": [],
   "source": [
    "label_map = get_map(train_name)\n",
    "label_key = list(label_map.keys())\n",
    "train_set = MyDataset(file_Path, train_name, label_map,\n",
    "                            train_transforms)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "valid_set = MyDataset(file_Path, test_name, label_map,\n",
    "                            val_transforms)\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset=valid_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JiIcaG1zSZLA",
    "outputId": "bd4d6e91-6c28-4b42-f242-246250dc55f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10325\n",
      "241\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF9yMaRrSvmv"
   },
   "source": [
    "## Effecient Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u5YZG1eozIv"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_select('ResNet18', num_input, 90).to(device)\n",
    "# print(model)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model_state_disk = torch.load('/home/a611/Projects/msk/fewshot_main/Model/pretrain/ResNet18/test_acc_0.766925_epoch_159', map_location = 'cpu')\n",
    "model.load_state_dict(model_state_disk)\n",
    "model = model.module\n",
    "model.linear = torch.nn.Linear(512, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "# print(model)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model_ = None\n",
    "# highest_test_acc = 0\n",
    "# for i in range(epochs):\n",
    "#     # print('EPOCH:', i + 1)\n",
    "#     train_iter = iter(train_loader)\n",
    "#     test_iter = iter(valid_loader)\n",
    "#     ########################################\n",
    "#     train_loss, train_acc = train(model, device, train_iter, optimizer, train_set, batch_size)\n",
    "#     test_loss, test_acc = test(model, device, test_iter, valid_set, batch_size)\n",
    "#     scheduler.step()\n",
    "#     print( 'EPOCH: %03d, train_loss: %3f, train_acc: %3f, test_loss: %3f, test_acc: %3f'\n",
    "#           % (i + 1, train_loss, train_acc, test_loss, test_acc))\n",
    "#     if test_acc > highest_test_acc:\n",
    "#         highest_test_acc = test_acc\n",
    "#         model_ = copy.deepcopy(model)\n",
    "#         print('Highest test accuracy: %3f' % highest_test_acc)\n",
    "#         torch.save(model_, '../models/CSE_train.model')\n",
    "#     print( 'EPOCH: %03d, train_loss: %3f, train_acc: %3f' % (i + 1, train_loss, train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../models/NICO_train.model', map_location = 'cpu')\n",
    "# print(model.layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(x, output, input_type):\n",
    "    output_max = output.argmax(dim=1)\n",
    "    output_numpy = output.cpu().detach().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    y = output_numpy[0]\n",
    "    ax.bar(x = x, height = y)\n",
    "    ax.set_title('Item: %s.' % x[int(output_max)], fontsize=15);\n",
    "    xticks = ax.get_xticks()\n",
    "    for i in range(len(y)):\n",
    "        xy = (xticks[i], y[i])\n",
    "        s = '%03f' % y[i]\n",
    "        ax.annotate(\n",
    "            text=s,  # 要添加的文本\n",
    "            xy=xy,  # 将文本添加到哪个位置\n",
    "            fontsize=8,  # 标签大小\n",
    "            color=\"red\",  # 标签颜色\n",
    "            ha=\"center\",  # 水平对齐\n",
    "            va=\"baseline\"  # 垂直对齐\n",
    "        )\n",
    "        plt.savefig(\"../results/%s_%s.jpg\" % (input_type, x[int(output_max)]))\n",
    "    return output_max\n",
    "\n",
    "def result(x, output, input_type):\n",
    "    output_max = output.argmax(dim=1)\n",
    "    print('Input: %s; Item: %s.' % (input_type, x[int(output_max)]))\n",
    "    return int(output_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = pd.read_csv(test_name[0], header=None)\n",
    "\n",
    "# for i in range(len(test_list)):\n",
    "#     print(test_list[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█████████████▌                                                                                                                | 26/241 [00:13<01:47,  2.00it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m test_transforms(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 41\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# output_max = draw(label_key, output)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m predict_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/ViT/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/gyc/Local_Features/classifier/Resnet.py:94\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 94\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     95\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[1;32m     96\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n",
      "File \u001b[0;32m~/.conda/envs/ViT/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/ViT/lib/python3.9/site-packages/torch/nn/modules/conv.py:446\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ViT/lib/python3.9/site-packages/torch/nn/modules/conv.py:442\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    440\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    441\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 442\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "device = 'cpu'\n",
    "model.eval()\n",
    "model.to(device)\n",
    "target_layers = [model.layer4[-1]]\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "#########################################\n",
    "# # for input_file_name in os.listdir('inputs'):\n",
    "# input_file_name = 'doll_86.jpg'\n",
    "# image_name = os.path.join('inputs',input_file_name)\n",
    "# image = Image.open(image_name)\n",
    "# img = image.resize((224, 224))\n",
    "# input = test_transforms(image).unsqueeze(0)\n",
    "# input = input.to(device)\n",
    "# output = model(input)\n",
    "# # output_max = draw(label_key, output)\n",
    "# output_max = result(label_key, output, 'origin')\n",
    "# output_max = draw(label_key, output, 'origin')\n",
    "total = 0\n",
    "acc = 0\n",
    "if not os.path.exists('../NICO_IID/'):\n",
    "    os.makedirs('../NICO_IID/')\n",
    "    \n",
    "with open('../NICO_IID/log.csv', 'w') as logfile:\n",
    "    logfile.writelines('File Path,Real Label,Predict Label,Sub Label\\n')\n",
    "    for i in tqdm(range(len(test_list))):\n",
    "        item = test_list[0][i]\n",
    "        split = re.split('[/.]', item)\n",
    "        class_name, subclass_name, pic_name = (split[1], split[2], split[3])\n",
    "\n",
    "        if not os.path.exists('../NICO_IID/%s/%s/' % (class_name, subclass_name)):\n",
    "            os.makedirs('../NICO_IID/%s/%s/' % (class_name, subclass_name))\n",
    "\n",
    "        real_label = test_list[1][i]\n",
    "        image_name = os.path.join(file_Path, item)\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        img = image.resize((224, 224))\n",
    "        input = test_transforms(image).unsqueeze(0)\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "        # output_max = draw(label_key, output)\n",
    "        predict_index = int(output.argmax(dim=1))\n",
    "        #         output_max = draw(label_key, output, '%s, %s.' %(catagory, item))\n",
    "        real_index = label_key.index(real_label)\n",
    "        predict_label = label_key[predict_index]\n",
    "    #     print(predict_index)\n",
    "\n",
    "\n",
    "#         predict_index = result(label_key, output, '%s.' % item)\n",
    "        targets = [ClassifierOutputTarget(predict_index)]\n",
    "        grayscale_cam = cam(input_tensor=input, targets = targets)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        image = np.array(image, dtype=np.float32) / 255\n",
    "        grayscale_cam = cv2.resize(grayscale_cam, (image.shape[1], image.shape[0]))\n",
    "        visualization = show_cam_on_image(image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        im = Image.fromarray(visualization)\n",
    "\n",
    "    #         im.save(\"../output.jpg\")\n",
    "\n",
    "        im.save('../NICO_IID/%s/%s/%s_%s.jpg' % (class_name, subclass_name, pic_name, predict_label))\n",
    "\n",
    "#             time.sleep(5)\n",
    "        if predict_label == real_label:\n",
    "            acc += 1\n",
    "        total += 1\n",
    "        logfile.write('%s,%s,%s,%s\\n' % (item, real_label, predict_label, subclass_name))\n",
    "    if total:\n",
    "        print('----------------------{} / {}, Acc: {}%. ----------------------'.format(acc, total, format(acc/total * 100, '.2f')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_file_name = \"/home/a611/Projects/Datasets/NICO/Animal/bear/on snow/15.png\"\n",
    "# error_image = Image.open(error_file_name)\n",
    "# error_image = np.array(error_image)\n",
    "# print(error_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db547acdbe0d8c01ab0f92785cee0c5ffdcae5ee11abcdf1f9927c721543542e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
