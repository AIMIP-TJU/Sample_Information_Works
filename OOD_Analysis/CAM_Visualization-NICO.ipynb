{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorwatch as tw\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils.dataset import *\n",
    "import torch.utils.data as data\n",
    "\n",
    "from utils.train import *\n",
    "from utils.test  import *\n",
    "from utils.model_select import model_select\n",
    "\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "####### test作训练集 #######\n",
    "# epochs = 80\n",
    "# lr = 3e-6\n",
    "# gamma = 0.7\n",
    "# step_size = 5\n",
    "###### train作训练集 #######\n",
    "epochs = 5\n",
    "lr = 2e-6\n",
    "gamma = 0.8\n",
    "step_size = 2\n",
    "\n",
    "seed = 42\n",
    "device = 'cpu'\n",
    "\n",
    "file_Path = '/home/a611/Projects/Datasets/NICO/'\n",
    "train_name = ['/home/a611/Projects/Datasets/NICO/labels/NICO_animal_train.csv']\n",
    "test_name = ['/home/a611/Projects/gyc/Local_Features/csvs/NICO_animal_test_with_cage_with_dog_eating_without_monkey.csv']\n",
    "num_classes = 10\n",
    "num_input = 3\n",
    "batch_size = 8\n",
    "num_workers = 8\n",
    "########################\n",
    "os.chdir('examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEIyZaYXRkZF"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YKR5RaykR58x"
   },
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b0JAXWWtSXpB"
   },
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pC-D9a8xSZNd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bear', 'bird', 'cat', 'cow', 'dog', 'elephant', 'horse', 'monkey', 'rat', 'sheep']\n"
     ]
    }
   ],
   "source": [
    "label_map = get_map(train_name)\n",
    "label_key = list(label_map.keys())\n",
    "train_set = MyDataset(file_Path, train_name, label_map,\n",
    "                            train_transforms)\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "valid_set = MyDataset(file_Path, test_name, label_map,\n",
    "                            val_transforms)\n",
    "valid_loader = data.DataLoader(\n",
    "    dataset=valid_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "print(label_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JiIcaG1zSZLA",
    "outputId": "bd4d6e91-6c28-4b42-f242-246250dc55f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291\n",
      "338\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(valid_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TF9yMaRrSvmv"
   },
   "source": [
    "## Effecient Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u5YZG1eozIv"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = model_select('ResNet18', num_input, 90).to(device)\n",
    "# print(model)\n",
    "model = torch.nn.DataParallel(model.to(device))\n",
    "checkpoint = torch.load('/home/a611/Projects/gyc/fewshot/model/pretrained/test_acc_0.766925_epoch_159', map_location = device)\n",
    "model.load_state_dict(checkpoint)\n",
    "model.module.linear = nn.Linear(512, num_classes).to(device)\n",
    "######################## 下边这个！！！！ ########################\n",
    "pth_file = '/home/a611/Projects/gyc/Local_Features/models/without_cage_without_dog_eating_without_monkey_ResNet18_random_1_0.01/test_acc_0.918826_epoch_59'\n",
    "model_state_dict = torch.load(pth_file, map_location = device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model = model.module\n",
    "model = model.to(device)\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# model.load_state_dict(torch.load(pth_file, map_location = device))\n",
    "\n",
    "# model.module.linear = nn.Linear(512 , num_classes)\n",
    "# model = model.module\n",
    "# model = model.to(device)\n",
    "# print(model)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler\n",
    "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print(model)\n",
    "# print(model.layer4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(x, output, input_type):\n",
    "    output_max = output.argmax(dim=1)\n",
    "    output_numpy = output.cpu().detach().numpy()\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    y = output_numpy[0]\n",
    "    ax.bar(x = x, height = y)\n",
    "    ax.set_title('Item: %s.' % x[int(output_max)], fontsize=15);\n",
    "    xticks = ax.get_xticks()\n",
    "    for i in range(len(y)):\n",
    "        xy = (xticks[i], y[i])\n",
    "        s = '%03f' % y[i]\n",
    "        ax.annotate(\n",
    "            text=s,  # 要添加的文本\n",
    "            xy=xy,  # 将文本添加到哪个位置\n",
    "            fontsize=8,  # 标签大小\n",
    "            color=\"red\",  # 标签颜色\n",
    "            ha=\"center\",  # 水平对齐\n",
    "            va=\"baseline\"  # 垂直对齐\n",
    "        )\n",
    "        plt.savefig(\"../results/%s_%s.jpg\" % (input_type, x[int(output_max)]))\n",
    "    return output_max\n",
    "\n",
    "def result(x, output, input_type):\n",
    "    output_max = output.argmax(dim=1)\n",
    "    print('Input: %s; Item: %s.' % (input_type, x[int(output_max)]))\n",
    "    return int(output_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = pd.read_csv(test_name[0], header=None)\n",
    "\n",
    "# for i in range(len(test_list)):\n",
    "#     print(test_list[0][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9420dfbf66a8417e90b8506d81b69f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2701 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Acc: 73.97%. ----------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "model.eval()\n",
    "target_layers = [model.layer4[-1]]\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\n",
    "#########################################\n",
    "# # for input_file_name in os.listdir('inputs'):\n",
    "# input_file_name = 'doll_86.jpg'\n",
    "# image_name = os.path.join('inputs',input_file_name)\n",
    "# image = Image.open(image_name)\n",
    "# img = image.resize((224, 224))\n",
    "# input = test_transforms(image).unsqueeze(0)\n",
    "# input = input.to(device)\n",
    "# output = model(input)\n",
    "# # output_max = draw(label_key, output)\n",
    "# output_max = result(label_key, output, 'origin')\n",
    "# output_max = draw(label_key, output, 'origin')\n",
    "total = 0\n",
    "acc = 0\n",
    "\n",
    "dataset_type_path = '../NICO_OOD/%s/' % (test_name[0][61:-4] if len(test_name[0][61:-4]) else 'origin')\n",
    "if not os.path.exists(dataset_type_path):\n",
    "    os.makedirs(dataset_type_path)\n",
    "    \n",
    "with open('../logs/log_%s.csv' % (test_name[0][61:-4] if len(test_name[0][61:-4]) else 'origin'), 'w') as logfile:\n",
    "    logfile.writelines('File Path,Real Label,Predict Label,Sub Label\\n')\n",
    "    for i in tqdm(range(len(test_list))):\n",
    "        item = test_list[0][i]\n",
    "        split = re.split('[/.]', item)\n",
    "        class_name, subclass_name, pic_name = (split[1], split[2], split[3])\n",
    "\n",
    "        if not os.path.exists('%s%s/%s/wrong/' % (dataset_type_path, class_name, subclass_name)):\n",
    "            os.makedirs('%s%s/%s/wrong/' % (dataset_type_path, class_name, subclass_name))\n",
    "        if not os.path.exists('%s%s/%s/right/' % (dataset_type_path, class_name, subclass_name)):\n",
    "            os.makedirs('%s%s/%s/right/' % (dataset_type_path, class_name, subclass_name))\n",
    "            \n",
    "        real_label = test_list[1][i]\n",
    "        image_name = os.path.join(file_Path, item)\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        img = image.resize((224, 224))\n",
    "        input = test_transforms(image).unsqueeze(0)\n",
    "        input = input.to(device)\n",
    "        output = model(input)\n",
    "#         print(output)\n",
    "        # output_max = draw(label_key, output)\n",
    "        predict_index = int(output.argmax(dim=1))\n",
    "#         print(predict_index)\n",
    "        #         output_max = draw(label_key, output, '%s, %s.' %(catagory, item))\n",
    "        real_index = label_key.index(real_label)\n",
    "        predict_label = label_key[predict_index]\n",
    "#         print(real_label, predict_label)\n",
    "    #     print(predict_index)\n",
    "\n",
    "        targets = [ClassifierOutputTarget(predict_index)]\n",
    "        grayscale_cam = cam(input_tensor=input, targets = targets)\n",
    "        grayscale_cam = grayscale_cam[0, :]\n",
    "        image = np.array(image, dtype=np.float32) / 255\n",
    "        grayscale_cam = cv2.resize(grayscale_cam, (image.shape[1], image.shape[0]))\n",
    "        visualization = show_cam_on_image(image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "        im = Image.fromarray(visualization)\n",
    "        \n",
    "        if predict_label != real_label:\n",
    "\n",
    "            im.save('%s%s/%s/wrong/%s_%s.jpg' % (dataset_type_path, class_name, subclass_name, pic_name, predict_label))\n",
    "\n",
    "        if predict_label == real_label:\n",
    "            \n",
    "            im.save('%s%s/%s/right/%s_%s.jpg' % (dataset_type_path, class_name, subclass_name, pic_name, predict_label))\n",
    "            acc += 1\n",
    "            \n",
    "        total += 1\n",
    "        logfile.write('%s,%s,%s,%s\\n' % (item, real_label, predict_label, subclass_name))\n",
    "    if total:\n",
    "        print('---------------------- Acc: {}%. ----------------------'.format(format(acc/total * 100, '.2f')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_file_name = \"/home/a611/Projects/Datasets/NICO/Animal/bear/on snow/15.png\"\n",
    "# error_image = Image.open(error_file_name)\n",
    "# error_image = np.array(error_image)\n",
    "# print(error_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db547acdbe0d8c01ab0f92785cee0c5ffdcae5ee11abcdf1f9927c721543542e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
